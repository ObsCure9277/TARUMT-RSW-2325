{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35ABic3Ak02z"
   },
   "source": [
    "# <font color=\"maroon\"> 1.0 NLP Toolkits and Preprocessing Techniques </font>\n",
    "\n",
    "### NLP Toolkits\n",
    "▪ Python libraries for natural language processing\n",
    "\n",
    "### Text Preprocessing Techniques\n",
    "▪ Converting text to a meaningful format for analysis\n",
    "▪ Preprocessing and cleaning text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V62vdfInk022"
   },
   "source": [
    "## Code: How to Install NLTK\n",
    "\n",
    "### Command Line\n",
    "pip install nlt\n",
    "\n",
    "### Jupyter Notebook\n",
    "import nltk\n",
    "nltk.download()\n",
    "\n",
    "#downloads all data & models\n",
    "#this will take a while\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLCozzHjk022"
   },
   "source": [
    "## Sample Text Data\n",
    "\n",
    "**Hi Mr. Smith! I am going to buy some vegetables (tomatoes and cucumbers) from the\n",
    "store. Should I pick up some black-eyed peas as well?**\n",
    "\n",
    "Text data is messy.\n",
    "\n",
    "To analyze this data, we need to preprocess the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApNgAjeqk023"
   },
   "source": [
    "![](https://i.imgur.com/pt5p6Hb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jBmgXb7k023"
   },
   "source": [
    "# Code: Tokenization (Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRtxZNZtV4Mc"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYjeWTf0k023",
    "outputId": "694cd797-ac89-437a-d0c1-c0ef0a999391"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables \\\n",
    "(2 tomatoes and 4 cucumbers from the store. Should I pick up some black-eyed peas as well?\"\n",
    "\n",
    "print(word_tokenize(my_text)) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzZ_SG_kk024"
   },
   "source": [
    "# Code: Tokenization (Sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wi834t7Ek024",
    "outputId": "a6f1139a-fef6-47ee-e99b-e34c9ecd5667",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "my_text = \"Hi Mr. Smith! I’m going to buy some vegetables \\\n",
    "(2 tomatoes and 4 cucumbers)from the store. Should I pick up some black-eyed peas as well?\"\n",
    "\n",
    "print(sent_tokenize(my_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dN9L0gGk025"
   },
   "source": [
    "# Code: Tokenization (Regular Expressions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvhK13m5k025"
   },
   "source": [
    "![](https://i.imgur.com/3L6x92C.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLRudDeLk026"
   },
   "source": [
    "# Code: Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMlRIFT8k026",
    "outputId": "19697998-02d1-4cad-9529-3d49bae9f676"
   },
   "outputs": [],
   "source": [
    "import re # Regular expression library\n",
    "import string\n",
    "# Replace punctuations with a white space\n",
    "#clean_text = re.sub('[%s]' % re.escape(string.punctuation), ' ', my_text)\n",
    "#clean_text\n",
    "\n",
    "s = re.sub('[^\\w\\s]','',my_text)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFSU428Ck026"
   },
   "source": [
    "# Code: Make All Text Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6hcxk1ak027"
   },
   "outputs": [],
   "source": [
    "clean_text = s.lower()\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uoaBcONOk027"
   },
   "source": [
    "# Code: Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5e8Ot44k027"
   },
   "outputs": [],
   "source": [
    "# Removes all words containing digits\n",
    "clean_text = re.sub('\\d', '', clean_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGP8X7cak027"
   },
   "source": [
    "# <font color='blue'>Preprocessing: Stop Words</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2SzAuxbk028"
   },
   "source": [
    "![](https://i.imgur.com/T5RJXrX.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GSGY5sik028"
   },
   "source": [
    "# Code: Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjdZhGrMk028"
   },
   "source": [
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBTaQAS-k028"
   },
   "source": [
    "# Code: Remove Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt8xtmfCk028"
   },
   "source": [
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">CountVectorizer</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3lBhjqJGk029",
    "outputId": "43624ce3-e43f-4fbd-8982-c9da79e48ca8"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "my_text = [\"Hi Mr. Smith! I’m going to buy some vegetables \\\n",
    "(2 tomatoes and 3 cucumbers from the store. Should I pick up some black-eyed peas as well?\"]\n",
    "           \n",
    "# Incorporate stop words when creating the count vectorizer\n",
    "cv = CountVectorizer(stop_words='english') \n",
    "X = cv.fit_transform(my_text)\n",
    "pd.DataFrame(X.toarray(), columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKr2I0-Jk029"
   },
   "source": [
    "![](https://i.imgur.com/9qllh8j.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkouSKlwk029"
   },
   "source": [
    "# Code: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-okFUv-k02-",
    "outputId": "d3f6ffe6-8c23-4adf-f9f6-e52e71e16045"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Try some stems\n",
    "print('drive: {}'.format(stemmer.stem('drive')))\n",
    "print('drives: {}'.format(stemmer.stem('drives')))\n",
    "print('driver: {}'.format(stemmer.stem('driver')))\n",
    "print('drivers: {}'.format(stemmer.stem('drivers')))\n",
    "print('driven: {}'.format(stemmer.stem('driven')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgyUyDZfk02_"
   },
   "source": [
    "![](https://i.imgur.com/8edVsCR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7etKdjjEk02_"
   },
   "source": [
    "# Code: Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IeXl-dNTk02_",
    "outputId": "e2964c96-a7b0-40c7-a25c-cd4361cd29a0"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "input_str=\"been had done languages cities mice\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icdQoHrrk03A"
   },
   "source": [
    "# Code: Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NiFKoaQ2k03A",
    "outputId": "c254a032-38a3-4984-f9e5-8afe5d42fa42"
   },
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "\n",
    "tokens = pos_tag(word_tokenize(my_text))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRjifONik03A"
   },
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6xqPDx0k03A"
   },
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "my_text = \"James Smith lives in the United States.\"\n",
    "tokens = pos_tag(word_tokenize(my_text)) # this labels each word as a part of speech\n",
    "entities = ne_chunk(tokens) # this extracts entities from the list of words\n",
    "entities.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3mdUZ4Mk03A"
   },
   "source": [
    "# <font color=\"blue\"> Prepocessing: Compound Term Extraction </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1Qw0PuWk03B"
   },
   "source": [
    "![](https://i.imgur.com/q1WuWai.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QH4qmhFVk03B"
   },
   "source": [
    "# Code: Compound Term Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AGm55Qnk03B"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer # multi-word expression\n",
    "\n",
    "my_text = \"You all are the greatest students of all time.\"\n",
    "\n",
    "mwe_tokenizer = MWETokenizer([('You','all'), ('of', 'all', 'time')])\n",
    "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(my_text))\n",
    "\n",
    "mwe_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot21kY7mk03B"
   },
   "source": [
    "![](https://i.imgur.com/HpgLFOT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oqFOp9nk03B"
   },
   "source": [
    "# Basic Pandas Functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYG95Kw0k03C"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame(np.random.randn(6,4))\n",
    "data = pd.read_csv('cookie_reviews.csv')\n",
    "#data\n",
    "\n",
    "#Selecting top and bottom rows:\n",
    "#Returns the first n rows.\n",
    "first = data  \n",
    "s=first.head()\n",
    "#Returns the last n rows.\n",
    "first = data  \n",
    "e=first.tail()\n",
    "\n",
    "print(s)\n",
    "print(e)\n",
    "\n",
    "#Selecting columns:\n",
    "#data['column_name'] \n",
    "#or data.column_name\n",
    "col_names=data.columns\n",
    "print(\"\\n column names = \",col_names)\n",
    "print (\"\\n \\n\")\n",
    "#Selecting by indexer:\n",
    "data.iloc[0] #- first row of data frame\n",
    "#data.iloc[-1] #- last row of data frame\n",
    "#data.iloc[:,0] #- first column of data frame\n",
    "#data.iloc[:,-1] #- last column of data frame\n",
    "#Data.iloc[0,1] #– first row, second column of the dataframe\n",
    "#data.iloc[0:4, 3:5] # first 4 rows and 3rd, 4th, 5th columns of data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8XgUv_3k03C"
   },
   "source": [
    "![](https://i.imgur.com/w9gWcfX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhCF0ibbk03C"
   },
   "outputs": [],
   "source": [
    "# Basic example\n",
    "square_me=lambda x: x*x\n",
    "\n",
    "my_numbers=[9, 3, 4, 100, 2, 1]\n",
    "my_numbers_squared = list(map(square_me, my_numbers))#map=applies a function to all the items in an input_list\n",
    "print(my_numbers_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73NlJvM_k03D"
   },
   "source": [
    "# <font color=red>Preprocessing Exercise </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUZr8d85k03D"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "We will be using review data from Kaggle to practice preprocessing text data. The dataset contains user reviews for many products, but today we'll be focusing on the product in the dataset that had the most reviews - an oatmeal cookie.\n",
    "\n",
    "The following code will help you load in the data. If this is your first time using nltk, you'll to need to pip install it first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49_8RmB-k03D"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() <-- Run this if it's your first time using nltk to download all of the datasets and models\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nSVd6X_k03D"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('cookie_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhpoyqpfk03E"
   },
   "source": [
    "**Question 1:**\n",
    "\n",
    "Determine how many reviews there are in total.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9Dz0atok03E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l63g7P8fk03E"
   },
   "source": [
    "**Question 2:**\n",
    "    \n",
    "Determine the percentage of 1, 2, 3, 4 and 5 star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BKYT60ck03E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9frnQ720k03E"
   },
   "source": [
    "**Question 3:**\n",
    "\n",
    "(a) Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqY6OLimk03E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uubKAy_ak03F"
   },
   "source": [
    "(b) Change to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Be2IxAEfk03F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mTwPsiFk03F"
   },
   "source": [
    "(b) Perform stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FjFvaoJzk03F"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv9e74jDk03F"
   },
   "source": [
    "# <font color=\"maroon\"> 2.0 Text Similarity Measures </font>\n",
    "\n",
    "- To measure distance between 2 string\n",
    "\n",
    "## 2.1 Applications\n",
    "- Information retrieval\n",
    "- Text classification\n",
    "- Document clustering\n",
    "- Topic Modeling\n",
    "- Matric decomposition\n",
    "\n",
    "To measure the word similarity, we use **<font color=\"blue\"> Levenshtein distance </font>**.\n",
    "- Minimum number of operations to get from one word to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaG7KOEik03F"
   },
   "source": [
    "![](https://i.imgur.com/FkdJmPi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1WK5fP5k03G"
   },
   "source": [
    "# TextBlob\n",
    "\n",
    "### Another toolkit other than NLTK\n",
    "\n",
    "- Wraps around NLTK and makes it easier to use\n",
    "\n",
    "### TextBlob capabilities\n",
    "\n",
    "- Tokenization\n",
    "- Parts of speech tagging\n",
    "- Sentiment analysis\n",
    "- Spell check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUlBT04yk03G"
   },
   "source": [
    "# TextBlob Demo: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1571,
     "status": "error",
     "timestamp": 1625705228446,
     "user": {
      "displayName": "CHING PANG GOH",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhAtxw1eyRJoJoQwsqDiWRYXwwRLKTenuEJ5Ge2vQ=s64",
      "userId": "03150219032412111071"
     },
     "user_tz": -480
    },
    "id": "lYml12fGk03G",
    "outputId": "d7a4566b-87a1-49a9-9ea6-3baa8094f151"
   },
   "outputs": [],
   "source": [
    "#pip install textblob\n",
    "\n",
    "from textblob import TextBlob\n",
    "my_text = TextBlob(\"We're moving from NLTK to TextBlob. How fun!\")\n",
    "my_text.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFMRGAVck03G"
   },
   "source": [
    "# TextBlob Demo: Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "erc2XHNJk03G",
    "outputId": "4d6a0d9b-bf42-4845-b44d-ddf5c133d05b"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"I'm graat at speling.\")\n",
    "print(blob.correct()) # print function requires Python 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LBe0AKSCk03H"
   },
   "source": [
    "<font color=\"blue\"> \n",
    "## How does the correct function work?  <br> \n",
    "    \n",
    "- Calculates the Levenshtein distance between the word ‘graat’ and all words in its word list </br>\n",
    "- Of the words with the smallest Levenshtein distance, it outputs the most popular word </br></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGGHwF-_k03H"
   },
   "source": [
    "# TextBlob Demo: Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBPNmxdLk03H",
    "outputId": "83ac4941-a17d-4fea-e0af-1d64863b88a3"
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(\"John hits the ball.\")\n",
    "for words, tag in blob.tags:\n",
    " print (words, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IjrZlWlk03H"
   },
   "source": [
    "# TextBlob Demo: Language Detection and Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMVDTiu2k03I",
    "outputId": "4a466581-6a16-475d-9977-a1949165b7a3"
   },
   "outputs": [],
   "source": [
    "word=TextBlob(\"Bonjour, comment allez-vous \")\n",
    "word.detect_language()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0Jk18Vjk03I",
    "outputId": "7a57f130-4a65-45d9-c7b5-6f0b0b40324e"
   },
   "outputs": [],
   "source": [
    "word.translate(from_lang='fr', to ='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9P_IjW4ok03I"
   },
   "source": [
    "# Text Format for Analysis: Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71HqDC9fk03I",
    "outputId": "f788a730-e2c0-445a-94da-645b67ce85a2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus =['This is the first document.', 'This is the second document.', 'And the third one. One is fun.'] #corpus=collection of teks\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus)\n",
    "pd.DataFrame(X.toarray(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Esi3yzSYk03J"
   },
   "source": [
    "![](https://i.imgur.com/OQDeQlb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GefIA4Jk03J"
   },
   "source": [
    "# Document Similarity: Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uSF6LaN3k03J"
   },
   "source": [
    "![](https://i.imgur.com/PyirXsy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iq-Cszhdk03J",
    "outputId": "bc2dd627-1839-4101-d733-a9842de3e019"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['The weather is hot under the sun',\n",
    "'I make my hot chocolate with milk',\n",
    "'One hot encoding',\n",
    "'I will have a chai latte with milk',\n",
    "'There is a hot sale today']\n",
    "# create the document-term matrix with count vectorizer\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "dt = pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reTZir8kk03K"
   },
   "source": [
    "# Document Similarity: Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLI6INUvk03K",
    "outputId": "7df91d90-1c96-4ab5-bc71-143d623be916"
   },
   "outputs": [],
   "source": [
    "# calculate the cosine similarity between all combinations of documents\n",
    "from itertools import combinations\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# list all of the combinations of 5 take 2 as well as the pairs of phrases\n",
    "pairs = list(combinations(range(len(corpus)),2)) #sentence (0, 1), (0, 2), (0, 3), (0, 4), (1, 2), (1, 3), .., (3,4))\n",
    "print(pairs)\n",
    "combos = [(corpus[a_index], corpus[b_index]) for (a_index, b_index) in pairs]\n",
    "print (combos)\n",
    "\n",
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results = [cosine_similarity([X[a_index]], [X[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results, combos), reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZbnjB3Hk03K",
    "outputId": "c8186453-7535-4086-9499-e394d20da6fb"
   },
   "outputs": [],
   "source": [
    "pairs = list(combinations(range(5),2))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63h3cPxxk03N"
   },
   "source": [
    "![](https://i.imgur.com/jrfN6Jj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdYuTyMkk03N"
   },
   "source": [
    "![](https://i.imgur.com/BI8XP92.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysNlUOUck03N"
   },
   "source": [
    "![](https://i.imgur.com/3IbfQXT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaYfQGbok03N"
   },
   "source": [
    "![](https://i.imgur.com/pnNqzql.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOxJucLNk03N",
    "outputId": "bbf2c1d1-89ec-4a6a-fa3a-2ab044b7e427"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = ['This is the first document.',\n",
    "         'This is the second document.',\n",
    "         'And the third one. One is fun.']\n",
    "\n",
    "\n",
    "# original Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X, columns=cv.get_feature_names())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwU3Q63Bk03O",
    "outputId": "5f405d59-5a48-4988-89c3-1cc8dcba43f9"
   },
   "outputs": [],
   "source": [
    "# new TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv_tfidf = TfidfVectorizer()\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X_tfidf, columns=cv_tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYpkjzzik03O"
   },
   "source": [
    "![](https://i.imgur.com/xlJibKw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0dZnaIzk03O"
   },
   "source": [
    "## Document Similarity: Example with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Jwp0N7ok03O",
    "outputId": "66ab19c6-dd92-415e-b32c-63d8e477ff8e"
   },
   "outputs": [],
   "source": [
    "corpus = ['The weather is hot under the sun',\n",
    "'I make my hot chocolate with milk',\n",
    "'One hot encoding',\n",
    "'I will have a chai latte with milk',\n",
    "'There is a hot sale today']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# create the document-term matrix with TF-IDF vectorizer\n",
    "cv_tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "X_tfidf = cv_tfidf.fit_transform(corpus).toarray()\n",
    "dt_tfidf = pd.DataFrame(X_tfidf,columns=cv_tfidf.get_feature_names())\n",
    "dt_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K4-aRyIk03P",
    "outputId": "98b5cf38-511b-4b8d-bb11-973afa5aa989"
   },
   "outputs": [],
   "source": [
    "# calculate the cosine similarity for all pairs of phrases and sort by most similar\n",
    "results_tfidf = [cosine_similarity([X_tfidf[a_index]], [X_tfidf[b_index]]) for (a_index, b_index) in pairs]\n",
    "sorted(zip(results_tfidf, combos), reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abfHLHtzk03P"
   },
   "source": [
    "![](https://i.imgur.com/mj4J60v.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_344udsk03P"
   },
   "source": [
    "# <font color=red>Text Similarity Exercise</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04ubfzrMk03P"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using a song lyric dataset from Kaggle to identify songs with similar lyrics. The data set contains artists, songs and lyrics for 55K+ songs, but today we will be focusing on songs by one group in particular - The Beatles.\n",
    "\n",
    "The following code will help you load in the data and get set up for this exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwTzQeppk03Q"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5F7qQISVk03Q"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('songdata.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fx7fvF-mk03Q"
   },
   "source": [
    "# Question 1\n",
    "\n",
    "Apply the following preprocessing steps:\n",
    "\n",
    "- Note the '\\n' (new line) characters in the lyrics. Remove them using regular expressions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL66xQSJk03Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6UNCrJUk03Q"
   },
   "source": [
    "## Question 2\n",
    "\n",
    "(a) List all the rows with \"Imagine\" in the title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IRZO0iWyk03Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JYSXoK5k03R"
   },
   "source": [
    "## Question 3\n",
    "\n",
    "(a) Extract the first line of lyric out from the first song.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Trrywo8Xk03R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5QT9hM8k03R"
   },
   "source": [
    "(b) Find out the sentiment of the extracted lyric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1m6qTGrPk03R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj5ae7oJV4M2"
   },
   "source": [
    "# NLP Showcase\n",
    "** 1 Name Gender Classifier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49aZZqnIV4M2",
    "outputId": "bdda8853-178b-40c8-bdf0-92848dd65f4f"
   },
   "outputs": [],
   "source": [
    "# code to build a classifier to classify names as male or female\n",
    "# demonstrates the basics of feature extraction and model building\n",
    "\n",
    "names = [(name, 'male') for name in nltk.corpus.names.words(\"male.txt\")]\n",
    "names += [(name, 'female') for name in nltk.corpus.names.words(\"female.txt\")]\n",
    "\n",
    "def extract_gender_features(name):\n",
    "    name = name.lower()\n",
    "    features = {}\n",
    "    features[\"suffix\"] = name[-1:]\n",
    "    features[\"suffix2\"] = name[-2:] if len(name) > 1 else name[0]\n",
    "    features[\"suffix3\"] = name[-3:] if len(name) > 2 else name[0]\n",
    "    features[\"suffix4\"] = name[-4:] if len(name) > 3 else name[0]\n",
    "    #features[\"suffix5\"] = name[-5:] if len(name) > 4 else name[0]\n",
    "    #features[\"suffix6\"] = name[-6:] if len(name) > 5 else name[0]\n",
    "    features[\"prefix\"] = name[:1]\n",
    "    features[\"prefix2\"] = name[:2] if len(name) > 1 else name[0]\n",
    "    features[\"prefix3\"] = name[:3] if len(name) > 2 else name[0]\n",
    "    features[\"prefix4\"] = name[:4] if len(name) > 3 else name[0]\n",
    "    features[\"prefix5\"] = name[:5] if len(name) > 4 else name[0]\n",
    "    #features[\"wordLen\"] = len(name)\n",
    "    \n",
    "    #for letter in \"abcdefghijklmnopqrstuvwyxz\":\n",
    "    #    features[letter + \"-count\"] = name.count(letter)\n",
    "   \n",
    "    return features\n",
    "\n",
    "data = [(extract_gender_features(name), gender) for (name,gender) in names]\n",
    "\n",
    "import random\n",
    "random.shuffle(data)\n",
    "\n",
    "#print(data[:10])\n",
    "#print()\n",
    "#print(data[-10:])\n",
    "\n",
    "dataCount = len(data)\n",
    "trainCount = int(.8*dataCount)\n",
    "\n",
    "trainData = data[:trainCount]\n",
    "testData = data[trainCount:]\n",
    "bayes = nltk.NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "def classify(name):\n",
    "    label = bayes.classify(extract_gender_features(name))\n",
    "    print(\"name=\", name, \"classifed as=\", label)\n",
    "\n",
    "print(\"trainData accuracy=\", nltk.classify.accuracy(bayes, trainData))\n",
    "print(\"testData accuracy=\", nltk.classify.accuracy(bayes, testData))\n",
    "\n",
    "bayes.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iQhN_J2AV4M3",
    "outputId": "93c303a4-c8ef-4710-cf71-7de252d452fb"
   },
   "outputs": [],
   "source": [
    "# print gender classifier errors so we can design new features to identify the cases\n",
    "errors = []\n",
    "\n",
    "for (name,label) in names:\n",
    "    if bayes.classify(extract_gender_features(name)) != label:\n",
    "        errors.append({\"name\": name, \"label\": label})\n",
    "\n",
    "errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d6ZHCFyV4M3"
   },
   "source": [
    "# ** 2 Sentiment Analysis **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cDSd-48bV4M3",
    "outputId": "348b8611-a18d-44e4-f3dc-9fded23e14f8"
   },
   "outputs": [],
   "source": [
    "# movie reviews / sentiment analysis - part #1\n",
    "from nltk.corpus import movie_reviews as reviews\n",
    "import random\n",
    "\n",
    "docs = [(list(reviews.words(id)), cat)  for cat in reviews.categories() for id in reviews.fileids(cat)]\n",
    "random.shuffle(docs)\n",
    "\n",
    "print([ (len(d[0]), d[0][:2], d[1]) for d in docs[:10]])\n",
    "\n",
    "fd = nltk.FreqDist(word.lower() for word in reviews.words())\n",
    "topKeys = [ key for (key,value) in fd.most_common(2000)]\n",
    "print(topKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "au6303BHV4M3"
   },
   "outputs": [],
   "source": [
    "# movie reviews sentiment analysis - part #2\n",
    "import nltk\n",
    "\n",
    "def review_features(doc):\n",
    "    docSet = set(doc)\n",
    "    features = {}\n",
    "    \n",
    "    for word in topKeys:\n",
    "        features[word] = (word in docSet)\n",
    "        \n",
    "    return features\n",
    "\n",
    "#review_features(reviews.words(\"pos/cv957_8737.txt\"))\n",
    "\n",
    "data = [(review_features(doc), label) for (doc,label) in docs]\n",
    "\n",
    "dataCount = len(data)\n",
    "trainCount = int(.8*dataCount)\n",
    "\n",
    "trainData = data[:trainCount]\n",
    "testData = data[trainCount:]\n",
    "bayes2 = nltk.NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "print(\"train accuracy=\", nltk.classify.accuracy(bayes2, trainData))\n",
    "print(\"test accuracy=\", nltk.classify.accuracy(bayes2, testData))\n",
    "\n",
    "bayes2.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jZff8DzyV4M4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Practical 4 Natural Language Processing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
