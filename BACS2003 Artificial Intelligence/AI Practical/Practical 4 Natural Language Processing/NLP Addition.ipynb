{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FFe-tOg03yv"
   },
   "source": [
    "# Section A: Preprocessing of TextÂ¶\n",
    "\n",
    "We have done some text processing in Practical 2. This practical will continue processing the text so that we could do some work related to natural language processing (NLP) later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atXKbyYB03y2"
   },
   "source": [
    "## Beforehand...\n",
    "** 1.1 NLTK Setup  **\n",
    "   - NLTK is included with the Anaconda Distribution of Python, or can be downloaded directly from nltk.org. \n",
    "   - Once NLTK is installed, the text data files (corpora) should be downloaded.  See the following cell to start the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KjQK04Z03y2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# uncomment the line below to download NLTK resources the first time NLTK is used and RUN this cell.\n",
    "# when the \"NLTK Downloader\" dialog appears (takes 10-20 seconds), click on the \"download\" button \n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7368xyK03y3"
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "\n",
    "from __future__ import division\n",
    "import nltk, re\n",
    "from nltk import word_tokenize\n",
    "from nltk import regexp_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.draw import tree\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oiZUCHio03y4"
   },
   "source": [
    "Previously in Practical 1, we learnt to tokenize the text into individual words. Examine the output of the tokens below. You may see non-useful tokens in NLP, such as punctuation marks.\n",
    "\n",
    "Furthermore, plural and singular words are different for the computer although they have the same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJIqLqZ_03y4"
   },
   "source": [
    "### What's Tokenization?\n",
    "<img src=\"https://i.ibb.co/30VPq9D/Tokenization.jpg\" style=\"max-width:50%;\" alt=\"Tokenization\" border=\"0\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbQoVQNu03y5",
    "outputId": "2b376f0c-de62-4643-fab3-4db3b54f9a99"
   },
   "outputs": [],
   "source": [
    "raw = \"\"\"Python is delicious. Default taggers are assigning their tag to every single\n",
    "word, even words that have never been encountered before.\n",
    "Once you have accomplished small things, \n",
    "you may attempt great ones safely.\"\"\"\n",
    "\n",
    "# Tokenize the text\n",
    "Token1 = word_tokenize(raw) #Method 1\n",
    "print(Token1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVOrXWKX03y6"
   },
   "source": [
    "To remove the punctuation marks, we may split the text using Regex (regular expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Phsrqp1C03y6",
    "outputId": "ed6c9859-af0b-477e-af92-ac14e626d288"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text using regex_tokenize\n",
    "Token2 = regexp_tokenize(raw, pattern='\\w+') #Method 2\n",
    "\n",
    "# Split the text with regex '\\W+'\n",
    "Token3 = re.split(\"\\W+\",raw) #Method 3\n",
    "\n",
    "print('Token 2:', Token2)\n",
    "print('Token 3:', Token3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0bSGevM03y7"
   },
   "source": [
    "Morphology computes the base form of English word, by removing the differences of affix-prefix, plural-singular, uppercase-lowercase. We can use the Porter Stemmer algorithm to perform the morphological processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPgl9Y-K03y7"
   },
   "outputs": [],
   "source": [
    "def stem1(wordList):\n",
    "    p = nltk.PorterStemmer()\n",
    "    result = [p.stem(word) for word in wordList] #['s', 'o', 'c', ',', ' ', '3', '2', '\\n',...]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCI7OsaM03y7"
   },
   "source": [
    "We may also use Regex to extract the root of the words that end with 's', 'es', 'ing', etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TahOQMmi03y8"
   },
   "outputs": [],
   "source": [
    "def stem2(wordList):\n",
    "    output = []\n",
    "    regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n",
    "    for word in wordList:\n",
    "        stem, suffix = re.findall(regexp, word)[0]\n",
    "        output.append(stem)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz4c_fOE03y8"
   },
   "source": [
    "re.findall(regexp, word)[0]: Return all non-overlapping matches of pattern in string, as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6sezxOo03y8",
    "outputId": "de6be34a-4ddf-43b3-b0de-eea845363b3c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Token1\n",
    "text = nltk.Text(stem1(Token1[:-1])) # What does it mean by this [:-1] << face?\n",
    "print('Stem1, Token1', text[:]) # stem1 using Porter Stemmer, Token1(token with punctuation)\n",
    "\n",
    "print('****************************************************')\n",
    "\n",
    "text = nltk.Text(stem2(Token1[:-1]))\n",
    "print('Stem2, Token1', text[:]) # stem2 using RegEx, Token1(token with punctuation)\n",
    "\n",
    "# Question: Can you spot any differences between two stemmers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FebN1QuM03y9",
    "outputId": "b53d96f5-cf98-40e8-ea44-087c13865982"
   },
   "outputs": [],
   "source": [
    "#Token2\n",
    "text = nltk.Text(stem1(Token2[:-1])) # You should know what does it mean by this smiley face [:-1]\n",
    "print('Stem1, Token2', text[:]) # stem1 using Porter Stemmer, Token2(punctuation removed using regex_tokenize)\n",
    "\n",
    "print('****************************************************')\n",
    "\n",
    "text = nltk.Text(stem2(Token2[:-1])) \n",
    "print('Stem2, Token2', text[:]) # stem2 using RegEx, Token2(punctuation removed using regex_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvmnLzmS03y9",
    "outputId": "03d25083-1c80-468b-d1b1-3cc8e8ce4c86"
   },
   "outputs": [],
   "source": [
    "#Token3\n",
    "text = nltk.Text(stem1(Token3[:-1]))\n",
    "print('Stem1, Token3', text[:])  # stem1 using Porter Stemmer, Token3(token with punctuation removed using split function)\n",
    "\n",
    "print('****************************************************')\n",
    "\n",
    "text = nltk.Text(stem2(Token3[:-1]))\n",
    "print('Stem2, Token3', text[:])  # stem2 using RegEx, Token3(token with punctuation removed using split function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gzRUEC903y9"
   },
   "source": [
    "# Section B: Analyze the Structure of the Text\n",
    "<b>Categorization and Tagging</b>: Automatically tag the text using pos_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUcph1cv03y-",
    "outputId": "5b741915-bf80-421a-8190-0d0203ee97a5"
   },
   "outputs": [],
   "source": [
    "print(pos_tag(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NxZR0dq03y-"
   },
   "source": [
    "<b>Analyzing sentence structure:</b> Lets parse a sentence: <i>I shot an elephant in my pajamas</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeDzo2c703y-",
    "outputId": "d914cc49-61e8-43c8-a54c-22fa56961329",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    PP -> P NP\n",
    "    NP -> Det N | Det N PP | 'I'\n",
    "    VP -> V NP | VP PP\n",
    "    Det -> 'an' | 'my'\n",
    "    N -> 'elephant' | 'pajamas'\n",
    "    V -> 'shot'\n",
    "    P -> 'in'\n",
    "    \"\"\")\n",
    "sent = word_tokenize(\"I shot an elephant in my pajamas\")\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOLYh97c03y-"
   },
   "source": [
    "# Section C: Statistical Analysis using Simple Statistics\n",
    "Find words from string using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nFGUGACi03y_",
    "outputId": "904dde8a-e9e9-4919-d2b4-eecb5b79c6a1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to find the words that start with 'a'\n",
    "print(re.findall(r\"\\ba[\\w]*\", raw)) #\\b -- boundary between word and non-word\n",
    "\n",
    "print('****************************************************')\n",
    "\n",
    "# to find the words with at least 8 characters\n",
    "print(re.findall(r\"\\b\\w{8,}\\b\", raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtCVNdBH03y_"
   },
   "source": [
    "To find words from a list of words from Words Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62uCf93a03y_",
    "outputId": "a90befa3-eda5-4e25-c835-8d106ab4a3f5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wordList = [w for w in nltk.corpus.words.words('en') if w.islower()] #remove any proper names.\n",
    "print(wordList[:30])\n",
    "\n",
    "print('****************************************************')\n",
    "\n",
    "#Wondering what is the corpus looks like?\n",
    "print(nltk.corpus.words.words('en')[:50] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGNtrJhH03y_"
   },
   "source": [
    "We would like to determine the frequency of the meaningful words in a passage. In this case, we need to <b>exclude</b> all the stop words from the text. Stopwords usually have <b>little lexical content</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJrl9h2L03zA",
    "outputId": "8138bbcd-1ea4-42c6-b913-f9b451f5d0ad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the stop words in English that have little lexical content\n",
    "stopwords = stopwords.words('english')\n",
    "content = [w for w in text if w.lower() not in stopwords]\n",
    "for w in content:\n",
    "    print(w)\n",
    "content_fraction = len(content)/len(text)\n",
    "print(\"Lexical content\", content_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ai8gz6Mg03zB"
   },
   "source": [
    "# NLP Showcase\n",
    "## ** 1 Name Gender Classifier **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jv89ZArG03zB",
    "outputId": "d80e3f88-9640-4796-b37f-e9d40370da4a"
   },
   "outputs": [],
   "source": [
    "# code to build a classifier to classify names as male or female\n",
    "# demonstrates the basics of feature extraction and model building\n",
    "\n",
    "names = [(name, 'male') for name in nltk.corpus.names.words(\"male.txt\")]\n",
    "names += [(name, 'female') for name in nltk.corpus.names.words(\"female.txt\")]\n",
    "\n",
    "def extract_gender_features(name):\n",
    "    name = name.lower()\n",
    "    features = {}\n",
    "    features[\"suffix\"] = name[-1:]\n",
    "    features[\"suffix2\"] = name[-2:] if len(name) > 1 else name[0]\n",
    "    features[\"suffix3\"] = name[-3:] if len(name) > 2 else name[0]\n",
    "    features[\"suffix4\"] = name[-4:] if len(name) > 3 else name[0]\n",
    "    #features[\"suffix5\"] = name[-5:] if len(name) > 4 else name[0]\n",
    "    #features[\"suffix6\"] = name[-6:] if len(name) > 5 else name[0]\n",
    "    features[\"prefix\"] = name[:1]\n",
    "    features[\"prefix2\"] = name[:2] if len(name) > 1 else name[0]\n",
    "    features[\"prefix3\"] = name[:3] if len(name) > 2 else name[0]\n",
    "    features[\"prefix4\"] = name[:4] if len(name) > 3 else name[0]\n",
    "    features[\"prefix5\"] = name[:5] if len(name) > 4 else name[0]\n",
    "    #features[\"wordLen\"] = len(name)\n",
    "    \n",
    "    #for letter in \"abcdefghijklmnopqrstuvwyxz\":\n",
    "    #    features[letter + \"-count\"] = name.count(letter)\n",
    "   \n",
    "    return features\n",
    "\n",
    "data = [(extract_gender_features(name), gender) for (name,gender) in names]\n",
    "\n",
    "import random\n",
    "random.shuffle(data)\n",
    "\n",
    "#print(data[:10])\n",
    "#print()\n",
    "#print(data[-10:])\n",
    "\n",
    "dataCount = len(data)\n",
    "trainCount = int(.8*dataCount)\n",
    "\n",
    "trainData = data[:trainCount]\n",
    "testData = data[trainCount:]\n",
    "bayes = nltk.NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "def classify(name):\n",
    "    label = bayes.classify(extract_gender_features(name))\n",
    "    print(\"name=\", name, \"classifed as=\", label)\n",
    "\n",
    "print(\"trainData accuracy=\", nltk.classify.accuracy(bayes, trainData))\n",
    "print(\"testData accuracy=\", nltk.classify.accuracy(bayes, testData))\n",
    "\n",
    "bayes.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PripjVoj03zB"
   },
   "outputs": [],
   "source": [
    "# print gender classifier errors so we can design new features to identify the cases\n",
    "errors = []\n",
    "\n",
    "for (name,label) in names:\n",
    "    if bayes.classify(extract_gender_features(name)) != label:\n",
    "        errors.append({\"name\": name, \"label\": label})\n",
    "\n",
    "errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "is_TcZag03zC"
   },
   "source": [
    "## ** 2 Sentiment Analysis **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JegZuMP03zC"
   },
   "outputs": [],
   "source": [
    "# movie reviews / sentiment analysis - part #1\n",
    "from nltk.corpus import movie_reviews as reviews\n",
    "import random\n",
    "\n",
    "docs = [(list(reviews.words(id)), cat)  for cat in reviews.categories() for id in reviews.fileids(cat)]\n",
    "random.shuffle(docs)\n",
    "\n",
    "print([ (len(d[0]), d[0][:2], d[1]) for d in docs[:10]])\n",
    "\n",
    "fd = nltk.FreqDist(word.lower() for word in reviews.words())\n",
    "topKeys = [ key for (key,value) in fd.most_common(2000)]\n",
    "print(topKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qH4ZouFn03zD",
    "outputId": "293f9832-da9d-4afc-b442-62e2e50fe01e"
   },
   "outputs": [],
   "source": [
    "# movie reviews sentiment analysis - part #2\n",
    "import nltk\n",
    "\n",
    "def review_features(doc):\n",
    "    docSet = set(doc)\n",
    "    features = {}\n",
    "    \n",
    "    for word in topKeys:\n",
    "        features[word] = (word in docSet)\n",
    "        \n",
    "    return features\n",
    "\n",
    "#review_features(reviews.words(\"pos/cv957_8737.txt\"))\n",
    "\n",
    "data = [(review_features(doc), label) for (doc,label) in docs]\n",
    "\n",
    "dataCount = len(data)\n",
    "trainCount = int(.8*dataCount)\n",
    "\n",
    "trainData = data[:trainCount]\n",
    "testData = data[trainCount:]\n",
    "bayes2 = nltk.NaiveBayesClassifier.train(trainData)\n",
    "\n",
    "print(\"train accuracy=\", nltk.classify.accuracy(bayes2, trainData))\n",
    "print(\"test accuracy=\", nltk.classify.accuracy(bayes2, testData))\n",
    "\n",
    "bayes2.show_most_informative_features(20)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Practical 4 Natural Language Processing (202005).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
